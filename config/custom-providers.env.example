# ====================================================================
# AI Town - Custom LLM Providers Environment Configuration
# ====================================================================
# 
# This file demonstrates various configurations for custom LLM providers.
# Copy this file to `.env.local` and modify according to your needs.
#
# QUICK START:
# 1. Copy this file: cp config/custom-providers.env.example .env.local
# 2. Uncomment and configure the providers you want to use
# 3. Set DEFAULT_LLM_PROVIDER to your chosen provider
# 4. Restart your development server
#
# ====================================================================

# Default LLM Provider Selection
# Options: openai, anthropic, localai, vllm, oobabooga, groq, together, custom
DEFAULT_LLM_PROVIDER=localai

# ====================================================================
# PROVIDER 1: vLLM (OpenAI-Compatible High-Performance Server)
# ====================================================================
# vLLM is optimized for high-throughput inference with various open-source models
#
# Documentation: https://docs.vllm.ai/
# Setup: https://docs.vllm.ai/en/latest/getting_started/installation.html

VLLM_API_BASE_URL=http://localhost:8000/v1
VLLM_API_KEY=your-vllm-api-key-here
VLLM_MODEL=meta-llama/Llama-2-7b-chat-hf
VLLM_MAX_TOKENS=2048
VLLM_TEMPERATURE=0.7
VLLM_TOP_P=0.9

# vLLM Performance Optimizations
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_NUM_BATCHED_TOKENS=4096
VLLM_DISABLE_LOG_STATS=false

# vLLM Advanced Settings
VLLM_QUANTIZATION=awq
VLLM_KV_CACHE_DTYPE=fp16
VLLM_SWAP_SPACE=4

# ====================================================================
# PROVIDER 2: LocalAI (Self-Hosted OpenAI-Compatible API)
# ====================================================================
# LocalAI provides OpenAI-compatible API for running various open-source models locally
#
# Documentation: https://localai.io/
# GitHub: https://github.com/mudler/LocalAI

LOCALAI_API_BASE_URL=http://localhost:8080/v1
LOCALAI_API_KEY=your-localai-api-key-here
LOCALAI_MODEL=llama-2-7b-chat
LOCALAI_MAX_TOKENS=2048
LOCALAI_TEMPERATURE=0.7
LOCALAI_TOP_P=0.9

# LocalAI Configuration
LOCALAI_CONTEXT_SIZE=2048
LOCALAI_THREADS=4
LOCALAI_GPUS=1
LOCALAI_DEBUG=false

# LocalAI Model Gallery (optional)
LOCALAI_MODEL_GALLERY_URL=https://raw.githubusercontent.com/go-skynet/model-gallery/main/gallery.json

# ====================================================================
# PROVIDER 3: Oobabooga Text Generation WebUI
# ====================================================================
# Oobabooga provides a user-friendly interface for running various LLM models
#
# GitHub: https://github.com/oobabooga/text-generation-webui
# Setup: Run with --api --listen flags

OOBABOOGA_API_BASE_URL=http://localhost:7860/v1
OOBABOOGA_API_KEY=your-oobabooga-api-key-here
OOBABOOGA_MODEL=llama-2-7b-chat
OOBABOOGA_MAX_TOKENS=2048
OOBABOOGA_TEMPERATURE=0.7
OOBABOOGA_TOP_P=0.9

# Oobabooga Extensions
OOBABOOGA_USE_INSTRUCTION_TEMPLATE=true
OOBABOOGA_INSTRUCTION_TEMPLATE=Alpaca
OOBABOOGA_CHARACTER=Assistant
OOBABOOGA_YOUR_NAME=User

# ====================================================================
# PROVIDER 4: Groq (Ultra-Fast Cloud LLM Service)
# ====================================================================
# Groq provides extremely fast inference with optimized hardware
#
# Website: https://groq.com/
# API Docs: https://console.groq.com/docs/quickstart

GROQ_API_KEY=gsk_your-groq-api-key-here
GROQ_MODEL=mixtral-8x7b-32768
GROQ_MAX_TOKENS=2048
GROQ_TEMPERATURE=0.7
GROQ_TOP_P=0.9

# Groq Performance Settings
GROQ_REQUEST_TIMEOUT=30000
GROQ_MAX_RETRIES=3
GROQ_RETRY_DELAY=1000

# ====================================================================
# PROVIDER 5: Together AI (Cloud Platform for Open Source Models)
# ====================================================================
# Together AI provides access to various open-source models with optimized inference
#
# Website: https://www.together.ai/
# API Docs: https://docs.together.ai/docs/quickstart

TOGETHER_API_KEY=your-together-api-key-here
TOGETHER_MODEL=meta-llama/Llama-2-7b-chat-hf
TOGETHER_MAX_TOKENS=2048
TOGETHER_TEMPERATURE=0.7
TOGETHER_TOP_P=0.9

# Together AI Advanced Settings
TOGETHER_MAX_RETRIES=3
TOGETHER_REQUEST_TIMEOUT=60000
TOGETHER_STREAM=false

# ====================================================================
# PROVIDER 6: Custom HTTP Provider (Generic OpenAI-Compatible API)
# ====================================================================
# Use this for any other OpenAI-compatible API endpoint
#
# Examples: Hugging Face Inference API, Custom deployments, etc.

CUSTOM_API_BASE_URL=https://your-custom-endpoint.com/v1
CUSTOM_API_KEY=your-custom-api-key-here
CUSTOM_MODEL=your-model-name
CUSTOM_MAX_TOKENS=2048
CUSTOM_TEMPERATURE=0.7
CUSTOM_TOP_P=0.9

# Custom Provider Headers (optional - comma-separated)
# Example: Authorization:Bearer token,X-Custom-Header:value
CUSTOM_HEADERS=

# Custom Provider SSL Verification (set to false for self-signed certificates)
CUSTOM_VERIFY_SSL=true

# ====================================================================
# FALLBACK CONFIGURATION
# ====================================================================
# Configure fallback providers when the primary provider fails

# Enable fallback mechanism (true/false)
ENABLE_FALLBACK=true

# Fallback provider order (comma-separated, highest priority first)
FALLBACK_PROVIDERS=localai,vllm,groq,openai

# Fallback retry settings
FALLBACK_MAX_RETRIES=2
FALLBACK_RETRY_DELAY=2000
FALLBACK_BACKOFF_MULTIPLIER=2

# Fallback error conditions (comma-separated)
# Options: timeout, rate_limit, server_error, network_error, invalid_response
FALLBACK_ERROR_CONDITIONS=timeout,server_error,network_error

# ====================================================================
# PERFORMANCE OPTIMIZATION SETTINGS
# ====================================================================
# Global performance settings that apply to all providers

# Request timeouts (in milliseconds)
GLOBAL_REQUEST_TIMEOUT=30000
GLOBAL_CONNECT_TIMEOUT=10000

# Connection pooling
MAX_CONCURRENT_REQUESTS=5
MAX_KEEP_ALIVE_CONNECTIONS=10

# Response streaming
ENABLE_STREAMING=true
STREAM_CHUNK_SIZE=1024

# Caching
ENABLE_RESPONSE_CACHE=true
CACHE_TTL=300000
CACHE_MAX_SIZE=1000

# ====================================================================
# SECURITY CONFIGURATION
# ====================================================================
# Security settings for LLM provider connections

# API key encryption (recommended for production)
ENCRYPT_API_KEYS=true
ENCRYPTION_KEY=your-32-character-encryption-key-here

# Request/Response logging (be careful with sensitive data)
ENABLE_REQUEST_LOGGING=false
LOG_REQUEST_BODY=false
LOG_RESPONSE_BODY=false

# IP whitelist for local providers (comma-separated)
LOCAL_PROVIDER_IP_WHITELIST=127.0.0.1,::1

# Rate limiting
ENABLE_RATE_LIMITING=true
RATE_LIMIT_REQUESTS_PER_MINUTE=60
RATE_LIMIT_BURST_SIZE=10

# ====================================================================
# MONITORING AND OBSERVABILITY
# ====================================================================
# Settings for monitoring LLM provider performance and health

# Health checks
ENABLE_HEALTH_CHECKS=true
HEALTH_CHECK_INTERVAL=30000
HEALTH_CHECK_TIMEOUT=5000

# Metrics collection
ENABLE_METRICS=true
METRICS_INCLUDE_LATENCY=true
METRICS_INCLUDE_TOKEN_USAGE=true
METRICS_INCLUDE_ERROR_RATES=true

# Performance alerts
PERFORMANCE_ALERT_THRESHOLD_MS=5000
ERROR_RATE_ALERT_THRESHOLD=0.1

# ====================================================================
# MODEL-SPECIFIC CONFIGURATIONS
# ====================================================================
# Override global settings for specific models

# Chat models (generally higher temperature for creativity)
CHAT_MODEL_TEMPERATURE=0.8
CHAT_MODEL_TOP_P=0.95
CHAT_MODEL_MAX_TOKENS=4096

# Code generation models (lower temperature for consistency)
CODE_MODEL_TEMPERATURE=0.2
CODE_MODEL_TOP_P=0.8
CODE_MODEL_MAX_TOKENS=2048

# Analysis models (very low temperature for accuracy)
ANALYSIS_MODEL_TEMPERATURE=0.1
ANALYSIS_MODEL_TOP_P=0.5
ANALYSIS_MODEL_MAX_TOKENS=1024

# ====================================================================
# DEVELOPMENT AND DEBUGGING
# ====================================================================
# Settings for development environments

# Debug mode (enables verbose logging)
DEBUG_MODE=false
DEBUG_INCLUDE_HEADERS=false
DEBUG_INCLUDE_TIMING=true

# Mock responses for testing (useful when API is unavailable)
USE_MOCK_RESPONSES=false
MOCK_RESPONSE_DELAY=1000

# Test scenarios
ENABLE_TEST_SCENARIOS=false
TEST_SCENARIO_LATENCY_INJECTION=false
TEST_SCENARIO_ERROR_INJECTION=false

# ====================================================================
# EXAMPLE CONFIGURATIONS FOR COMMON SETUPS
# ====================================================================
# Uncomment one of these sections for common deployment patterns

# --- Example 1: High-Performance Local Setup with vLLM ---
# DEFAULT_LLM_PROVIDER=vllm
# VLLM_API_BASE_URL=http://localhost:8000/v1
# VLLM_API_KEY=sk-your-key-here
# VLLM_MODEL=meta-llama/Llama-2-13b-chat-hf
# VLLM_GPU_MEMORY_UTILIZATION=0.95
# VLLM_MAX_NUM_BATCHED_TOKENS=8192
# ENABLE_FALLBACK=true
# FALLBACK_PROVIDERS=vllm,groq,openai

# --- Example 2: Cloud-Based with Groq (Fastest Option) ---
# DEFAULT_LLM_PROVIDER=groq
# GROQ_API_KEY=gsk_your-groq-api-key-here
# GROQ_MODEL=mixtral-8x7b-32768
# GROQ_REQUEST_TIMEOUT=15000
# ENABLE_FALLBACK=true
# FALLBACK_PROVIDERS=groq,together,openai

# --- Example 3: Hybrid Local + Cloud Setup ---
# DEFAULT_LLM_PROVIDER=localai
# LOCALAI_API_BASE_URL=http://localhost:8080/v1
# LOCALAI_MODEL=llama-2-7b-chat
# ENABLE_FALLBACK=true
# FALLBACK_PROVIDERS=localai,groq,openai
# FALLBACK_ERROR_CONDITIONS=timeout,server_error

# --- Example 4: Development with Mock Responses ---
# DEFAULT_LLM_PROVIDER=custom
# USE_MOCK_RESPONSES=true
# MOCK_RESPONSE_DELAY=500
# DEBUG_MODE=true
# ENABLE_REQUEST_LOGGING=true

# ====================================================================
# TROUBLESHOOTING TIPS
# ====================================================================
#
# 1. CONNECTION ISSUES:
#    - Verify API endpoints are accessible: curl http://localhost:8000/v1/models
#    - Check firewall settings for local providers
#    - Ensure SSL certificates are valid for HTTPS endpoints
#
# 2. PERFORMANCE ISSUES:
#    - Adjust MAX_CONCURRENT_REQUESTS based on your hardware
#    - Enable response caching for repeated queries
#    - Use streaming for long responses
#
# 3. AUTHENTICATION ISSUES:
#    - Verify API keys are correct and active
#    - Check rate limits and quotas
#    - Ensure proper headers are being sent
#
# 4. MODEL COMPATIBILITY:
#    - Check if model supports chat completion format
#    - Verify model context size limits
#    - Ensure model is properly loaded in the provider
#
# ====================================================================